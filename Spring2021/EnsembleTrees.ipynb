{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "## Subsurface Machine Learning with Ensemble Tree Regressor Methods\n",
    "\n",
    "### Tree Bagging and Random Forest for Subsurface Modeling in Python\n",
    "\n",
    "#### Harry Khuc, Senior Undergraduate, The University of Texas at Austin\n",
    "\n",
    "#### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "\n",
    "##### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1) | [GeostatsPy](https://github.com/GeostatsGuy/GeostatsPy)\n",
    "\n",
    "### PGE 383 Exercise: Ensemble Tree Regressors for Subsurface Modeling in Python \n",
    "\n",
    "Here's a simple workflow, demonstration of tree bagging and random forest for subsurface modeling workflows. This should help you get started with building subsurface models that data analytics and machine learning. Here's some basic details about ensemble tree methods.  \n",
    "\n",
    "#### Ensemble Tree Methods\n",
    "\n",
    "Machine learning method for supervised learning for classification and regression analysis.  Here are some key aspects of random forest.\n",
    "\n",
    "**Prediction**\n",
    "\n",
    "* estimate a function $\\hat{f}$ such that we predict a response feature $Y$ from a set of predictor features $X_1,\\ldots,X_m$. \n",
    "\n",
    "* the prediction is of the form $\\hat{Y} = \\hat{f}(X_1,\\ldots,X_m)$ \n",
    "\n",
    "**Supervised Learning**\n",
    "\n",
    "* the response feature label, $Y$, is available over the training and testing data\n",
    "    \n",
    "**Based on an Ensemble of Decision Trees**    \n",
    "    \n",
    "These are the concepts related to decision tree.    \n",
    "   \n",
    "**Hiearchical, Binary Segmentation of the Feature Space**\n",
    "\n",
    "The fundamental idea is to divide the predictor space, $ùëã_1,\\ldots,X_m$, into $J$ mutually exclusive, exhaustive regions\n",
    "\n",
    "* **mutually exclusive** ‚Äì any combination of predictors only belongs to a single region, $R_j$\n",
    "\n",
    "* **exhaustive** ‚Äì all combinations of predictors belong a region, $R_j$, regions cover entire feature space (range of the variables being considered)\n",
    "\n",
    "For every observation in a region, $R_j$, we use the same prediction, $\\hat{Y}(R_j)$    \n",
    "\n",
    "For example predict production, $\\hat{Y}$, from porosity, ${X_1}$\n",
    "\n",
    "* given the data within a mD feature space, $X_1,\\ldots,X_m$, find that boundary maximizes the gap between the two categories\n",
    "\n",
    "* new cases are classified based on where they fall relative to this boundary \n",
    " \n",
    "**Proceedure for Tree Construction**\n",
    "\n",
    "The tree is constructed from the top down.  We begin with a single region that covers the entire feature space and then proceed with a sequence of splits.\n",
    "\n",
    "* **Scan All Possible Splits** over all regions and over all features.\n",
    "\n",
    "* **Greedy Optimization**  The method proceeds by finding the first segmentation (split) in any feature that minimizes the residual sum of squares of errors over all the training data $y_i$ over all of the regions $j = 1,\\ldots,J$.\n",
    "\n",
    "\\begin{equation}\n",
    "RSS = \\sum^{J}_{j=1} \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2\n",
    "\\end{equation}\n",
    "\n",
    "* **Stopping Criteria** is typically based on minimum number of training data in each region for a robust estimation and / or minimum reduction in RSS for the next split \n",
    "\n",
    "**Esemble Methods**\n",
    "\n",
    "Model testing accuracy is reduced by model variance.  Model variance may be reduced through averaging multiple good estimates.\n",
    "\n",
    "Recall that variance is reduced by averaging given independent, identically distributed sampling as predicted by standard error as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{\\bar{x}}^2 = \\frac{\\sigma^2_s}{n}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, we can reduce model variance through the utilization of an ensemble of models.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_{avg}(X_1,...,X_m) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}^b(X_1,...,X_m)\n",
    "\\end{equation}\n",
    "\n",
    "This method works well with trees and is applied with the **tree bagging** and **random forest** prediction methods.\n",
    "\n",
    "* the trees are allowed to grow deep and complicated (are not pruned)\n",
    "\n",
    "* each of the individual models, $\\hat{f}_{avg}(X_1,...,X_m)$, are complicated resulting in low model bias, but high model variance\n",
    "\n",
    "* the averaging over the ensemble of models will then mitigate this high model variance\n",
    "\n",
    "Therefore, overfit is not an issue as it is for decision trees.   \n",
    "\n",
    "**Tree Bagging**\n",
    "\n",
    "To build the ensemble of models we need multiple training datasets.  This is typically not available.\n",
    "\n",
    "* the solution is to **bootstrap** the entire dataset to build multiple bootstrap realizations of training data, $X_1^b,...,X_m^b$\n",
    "\n",
    "* a deep decision tree is fit to each realization of the training data, $\\hat{f}^b(X_1^b,...,X_m^b)$\n",
    "\n",
    "* a prediction estimate is calculated by each tree in the ensemble $Y^b =\\hat{f}^b(X_1^b,...,X_m^b)$\n",
    "\n",
    "* for **regression** the ensemble prediction is the average of the prediction from each member of the ensemble, $Y = \\frac{1}{B} \\sum_{b=1}^{B} Y^b$\n",
    "\n",
    "* for **classification** the ensemble prediction is the majority-rule of the ensemble classifications, $Y = argmax_k(Y^b_k)$\n",
    "\n",
    "**Out-of-Bag**\n",
    "\n",
    "With bootstrap resampling of the data, it can be shown that about 2/3 of the data will be included (in expectation) for each tree.  \n",
    "\n",
    "* therefore are 1/3 of the data (in expectation) unused in predicting each tree, these are know as out-of-bag observations\n",
    "\n",
    "* for every response feature observation, $y_{\\alpha}$,  there will be $\\frac{B}{3}$ out-of-bag predictions, $y^{*,b}_{\\alpha}$\n",
    "\n",
    "* we can average (for regression) these prediction to calculate a single out-of-bag prediction, $y^{*}_{\\alpha} = \\sum_{\\alpha = 1}^{\\frac{B}{3}} y^{*,b}_{\\alpha}$\n",
    "\n",
    "* we then calculate the out-of-bag mean square error (MSE)\n",
    "\n",
    "\\begin{equation}\n",
    "MSE_{OOB} = \\sum_{\\alpha = 1}^{\\frac{B}{3}} \\left[ y^{*}_{\\alpha} - y_{\\alpha} \\right]^2\n",
    "\\end{equation}\n",
    "\n",
    "**Interpretability**\n",
    "\n",
    "Compared to decision trees, the ensemble methods have reduced interpretability.  One tool to improve model interpretability is feature importance.\n",
    "\n",
    "We calculate variable importance through calculating the average of:\n",
    "\n",
    "* residual sum of square reduction for all splits involving each predictor feature for regression\n",
    "\n",
    "* the decrease in the Gini index for all splits involving each predictor feature for classification\n",
    "\n",
    "Both are standardized to sum to 1.0 over the features.\n",
    "\n",
    "**Random Forest**\n",
    "\n",
    "One issue with tree bagging is the trees in the ensemble may be highly correlated.\n",
    "\n",
    "* this occurs when there is a dominant predictor feature as it will always be applied to the top split(s), the result is all the trees in the ensemble are very similar (i.e. correlated)\n",
    "\n",
    "* with highly correlated trees, there is significantly less reduction in model variance with the ensemble\n",
    "\n",
    "* with random forest, for each split only $\\sqrt{m}$ (or some other reduced set) of predictor features are candidates (selected at random)\n",
    "\n",
    "* this forces each tree in the ensemble to evolve in disimilar manner \n",
    "\n",
    "##### Applications to Subsurface Modeling\n",
    "\n",
    "We will predict unconventional well production from a single petrophysical and geomechanical predictor feature\n",
    "\n",
    "##### Why Cover Ensemble Methods with Decision Trees?\n",
    "\n",
    "Here's some thoughts:\n",
    "\n",
    "* build on from easy to understand decision trees\n",
    "* demonstrate reduction in model variance through an ensemble approach\n",
    "* random forest is quite powerful and is a top performing machine learning method in various types of problems \n",
    "\n",
    "#### Workflow Goals\n",
    "\n",
    "Learn the basics of ensemble tree methods in python to segment facies given petrophysical properties. This includes:\n",
    "\n",
    "* Loading and visualizing sample data\n",
    "* Trying out bagging tree and random forest \n",
    "* Test and observe the model behavoir and prove concepts\n",
    "* Develop understanding of ensemble tree decision behavior through interactive plotting\n",
    "* Provide hands-on example of decision tree splitting with Python packages matplotlib and ipywidgets\n",
    "\n",
    "\n",
    "#### Objective \n",
    "\n",
    "In the PGE 383: Stochastic Subsurface Modeling class I want to provide hands-on experience with building subsurface modeling workflows. Python provides an excellent vehicle to accomplish this. I have coded a package called GeostatsPy with GSLIB: Geostatistical Library (Deutsch and Journel, 1998) functionality that provides basic building blocks for building subsurface modeling workflows. \n",
    "\n",
    "The objective is to remove the hurdles of subsurface modeling workflow construction by providing building blocks and sufficient examples. This is not a coding class per se, but we need the ability to 'script' workflows working with numerical methods.    \n",
    "\n",
    "#### Getting Started\n",
    "\n",
    "Here's the steps to get setup in Python with the GeostatsPy package:\n",
    "\n",
    "1. Install Anaconda 3 on your machine (https://www.anaconda.com/download/). \n",
    "2. From Anaconda Navigator (within Anaconda3 group), go to the environment tab, click on base (root) green arrow and open a terminal. \n",
    "3. In the terminal type: pip install geostatspy. \n",
    "4. Open Jupyter and in the top block get started by copy and pasting the code block below from this Jupyter Notebook to start using the geostatspy functionality. \n",
    "\n",
    "You will need to copy the data file to your working directory.  They are available here:\n",
    "\n",
    "* Tabular data - unconv_MV.csv found [here](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV.csv).\n",
    "\n",
    "There are exampled below with these functions. You can go here to see a list of the available functions, https://git.io/fh4eX, other example workflows and source code. \n",
    "\n",
    "#### Import Required Packages\n",
    "\n",
    "We will also need some standard packages. These should have been installed with Anaconda 3.\n",
    "\n",
    "#### Load the required libraries\n",
    "\n",
    "The following code loads the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                   # to set current working directory \n",
    "import math                                                 # basic calculations like square root\n",
    "from sklearn import tree                                    # tree program from scikit learn (package for machine learning)\n",
    "from sklearn.tree import _tree                              # for accessing tree information\n",
    "from sklearn import metrics                                 # measures to check our models\n",
    "from sklearn.tree import export_graphviz                    # graphical visualization of trees\n",
    "from sklearn.preprocessing import StandardScaler            # standardize variables to mean of 0.0 and variance of 1.0\n",
    "\n",
    "from sklearn.model_selection import cross_val_score         # cross validation methods\n",
    "from sklearn.tree import DecisionTreeRegressor              # decision tree method\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingRegressor               # bagging tree method\n",
    "from sklearn.ensemble import RandomForestRegressor          # random forest method\n",
    "\n",
    "import pandas as pd                                         # DataFrames and plotting\n",
    "import pandas.plotting as pd_plot\n",
    "import numpy as np                                          # arrays and matrix math\n",
    "import matplotlib.pyplot as plt                             # plotting\n",
    "from subprocess import check_call\n",
    "import geostatspy.GSLIB as GSLIB                            # geostatistics and spatial methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Additional Required Packages\n",
    "\n",
    "The following workflow has been made interactive through the use of ipywidgets.\n",
    "\n",
    "Instructions for installation of this package can be found [here](https://ipywidgets.readthedocs.io/en/latest/user_install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interactive                        # widgets and interactivity\n",
    "from ipywidgets import widgets                            # widgets and interactivity\n",
    "import matplotlib.pyplot as plt                           # plotting\n",
    "import numpy as np                                        # working with arrays\n",
    "from scipy.stats import triang                            # parametric distributions\n",
    "from scipy.stats import binom                             # parametric distributions\n",
    "from scipy.stats import norm                              # parametric distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing 'python -m pip install [package-name]'. More assistance is available with the respective package docs.  \n",
    "\n",
    "#### Declare functions\n",
    "\n",
    "Let's define a couple of functions to streamline plotting correlation matrices and visualization and test our regression models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(dataframe,size=10):                           # plots a graphical correlation matrix \n",
    "    corr = dataframe.corr()\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    im = ax.matshow(corr,vmin = -1.0, vmax = 1.0)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns);\n",
    "    plt.colorbar(im, orientation = 'vertical')\n",
    "    plt.title('Correlation Matrix')\n",
    "    \n",
    "def visualize_model(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,title,):# plots the data points and the decision tree prediction \n",
    "    n_classes = 10\n",
    "    cmap = plt.cm.RdYlBu\n",
    "    plot_step = 0.02\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=cmap,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n",
    "\n",
    "    im = plt.scatter(xfeature,yfeature,s=None, c=response, marker=None, cmap=cmap, norm=None, vmin=z_min, vmax=z_max, alpha=0.8, linewidths=0.3, verts=None, edgecolors=\"black\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xfeature.name)\n",
    "    plt.ylabel(yfeature.name)\n",
    "    cbar = plt.colorbar(im, orientation = 'vertical')\n",
    "    cbar.set_label(response.name, rotation=270, labelpad=20)\n",
    "    return Z\n",
    "\n",
    "def visualize_grid(Z,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,title,):# plots the data points and the decision tree prediction \n",
    "    n_classes = 10\n",
    "    cmap = plt.cm.RdYlBu\n",
    "    plot_step = 0.02\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=cmap,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n",
    "\n",
    "    im = plt.scatter(xfeature,yfeature,s=None, c=response, marker=None, cmap=cmap, norm=None, vmin=z_min, vmax=z_max, alpha=0.8, linewidths=0.3, verts=None, edgecolors=\"black\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xfeature.name)\n",
    "    plt.ylabel(yfeature.name)\n",
    "    cbar = plt.colorbar(im, orientation = 'vertical')\n",
    "    cbar.set_label(response.name, rotation=270, labelpad=20)\n",
    "    \n",
    "def check_model(model,xfeature,yfeature,response,title):    # plots the estimated vs. the actual  \n",
    "    predict_train = model.predict(np.c_[xfeature,yfeature])\n",
    "    plt.scatter(response,predict_train,s=None, c='red',marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=0.3, verts=None, edgecolors=\"black\")\n",
    "    plt.title(title); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')\n",
    "    plt.xlim(0,7000); plt.ylim(0,7000)\n",
    "    plt.arrow(0,0,7000,7000,width=0.02,color='black',head_length=0.0,head_width=0.0)\n",
    "    MSE = metrics.mean_squared_error(response,predict_train)\n",
    "    Var_Explained = metrics.explained_variance_score(response,predict_train)\n",
    "    cor = math.sqrt(metrics.r2_score(response,predict_train))\n",
    "    print('Mean Squared Error on Training = ', round(MSE,2),', Variance Explained =', round(Var_Explained,2),'Cor =', round(cor,2))\n",
    "\n",
    "def feature_sample(array, xmin, ymin, xstep, ystep, df_x, df_y, name): # sampling predictions from a feature space grid \n",
    "\n",
    "    if array.ndim != 2:\n",
    "        raise ValueError(\"Array must be 2D\")\n",
    "\n",
    "    if len(df_x) != len(df_y):\n",
    "        raise ValueError(\"x and y feature arrays must have equal lengths\")\n",
    "        \n",
    "    ny, nx = array.shape\n",
    "    df = pd.DataFrame()\n",
    "    v = []\n",
    "    nsamp = len(df_x)\n",
    "    for isamp in range(nsamp):\n",
    "        x = df_x.iloc[isamp]\n",
    "        y = df_y.iloc[isamp]\n",
    "        iy = min(ny - int((y - ymin) / ystep) - 1, ny - 1)\n",
    "        ix = min(int((x - xmin) / xstep), nx - 1)\n",
    "        v.append(array[iy, ix])\n",
    "    df[name] = v\n",
    "    return df    \n",
    "    \n",
    "def check_grid(grid,xmin,xmax,ymin,ymax,xfeature,yfeature,response,title):    # plots the estimated vs. the actual  \n",
    "    if grid.ndim != 2:\n",
    "        raise ValueError(\"Prediction array must be 2D\")\n",
    "    ny, nx = grid.shape\n",
    "    xstep = (xmax - xmin)/nx; ystep = (ymax-ymin)/ny \n",
    "    #predict_train = model.predict(np.c_[xfeature,yfeature])\n",
    "    predict_train = feature_sample(grid, xmin, ymin, xstep, ystep, xfeature, yfeature, 'sample')\n",
    "    plt.scatter(response,predict_train,s=None, c='red',marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=0.3, verts=None, edgecolors=\"black\")\n",
    "    plt.title(title); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')\n",
    "    plt.xlim(0,7000); plt.ylim(0,7000)\n",
    "    plt.arrow(0,0,7000,7000,width=0.02,color='black',head_length=0.0,head_width=0.0)\n",
    "    MSE = metrics.mean_squared_error(response,predict_train)\n",
    "    Var_Explained = metrics.explained_variance_score(response,predict_train)\n",
    "    cor = math.sqrt(metrics.r2_score(response,predict_train))\n",
    "    print('Mean Squared Error on Training = ', round(MSE,2),', Variance Explained =', round(Var_Explained,2),'Cor =', round(cor,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the working directory\n",
    "\n",
    "I always like to do this so I don't lose files and to simplify subsequent read and writes (avoid including the full address each time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:/Users/Khuc/MACHINELEARNING/PROJECT/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d4235a6adb1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"c:/Users/Khuc/MACHINELEARNING/PROJECT/\"\u001b[0m\u001b[1;33m)\u001b[0m                                       \u001b[1;31m# set the working directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'c:/Users/Khuc/MACHINELEARNING/PROJECT/'"
     ]
    }
   ],
   "source": [
    "os.chdir(\"c:/Users/Khuc/MACHINELEARNING/PROJECT/\")                                       # set the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to update the part in quotes with your own working directory and the format is different on a Mac (e.g. \"~/PGE\").  \n",
    "\n",
    "#### Read the data table\n",
    "\n",
    "First copy the \"unconv_MV.csv\" comma delimited file from https://github.com/GeostatsGuy/GeoDataSets to your working directory, then run this command to read the file into a DataFrame object (part of Pandas package).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv(\"unconv_MV.csv\")                      # load the comma delimited data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the first several rows of our data stored in a DataFrame so we can make sure we successfully loaded the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head(n=13)                                          # preview the first 13 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the well index and check the summary summary statistics.  We will also take the first 300 samples for training data and the remaining 700 data for testing data.\n",
    "\n",
    "* we use only 300 data for testing to increase the difficulty for demonstration\n",
    "\n",
    "* the samples are random order, so this is 100 random samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_test = my_data.iloc[300:,1:8]                     # extract 700 samples for testing (we want more challenge so we reduce the training data set size)\n",
    "my_data = my_data.iloc[0:300,1:8]                         # copy all rows and columns 1 through 8, note 0 column is removed\n",
    "my_data.describe().transpose()                            # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_test.describe().transpose()                         # summary statistics for the testing data     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good that we checked the summary statistics, because we have some negative values for brittleness and total organic carbon. The is physically imposible.  The values must be in error. We know the lowest possible values are 0.0, so we will truncate on 0.0.  We use the *get_numerical_data()* DataFrame member function to get a shallow copy of the data from the DataFrame.  Since it is a shallow copy, any changes we make to the copy are made to the data in the original DataFrame.  This allows us to apply this simple conditional statement to all the data values in the DataFrame all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = my_data._get_numeric_data()                           # get the numerical values\n",
    "num[num < 0] = 0                                            # truncate negative values to 0.0\n",
    "my_data.describe().transpose()                              # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has variables from 1,000 unconventional wells including well average porosity, log transform of permeability (to linearize the relationships with other variables), accoustic impedance (kg/m2s*10^6), brittness ratio (%), total organic carbon (%), vitrinite reflectance (%), and initial production 90 day average (MCFPD).  Note, the dataset is synthetic.\n",
    "\n",
    "#### Feature Ranges\n",
    "\n",
    "Let's set predictor and response feature ranges for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pormin = 5.0; pormax = 25.0                                 # set minumums and maximums for visualization \n",
    "brittlemin = 0.0; brittlemax = 100.0\n",
    "prodmin = 0.0; prodmax = 13000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the correlation matrix \n",
    "\n",
    "For multivariate analysis it is a good idea to check the correlation matrix.  We can calculate it and view it in the console with these commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.corrcoef(my_data, rowvar = False)\n",
    "print(np.around(corr_matrix,2))                             # print the correlation matrix to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the 1.0 diagonal resulting from the correlation of each variable with themselves.  \n",
    "\n",
    "Let's use our function declared above to make a graphical correlation matrix visualization.  This may inprove our ability to spot features.  It relies on the built in correlation matrix method with Numpy DataFrames and MatPlotLib for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(my_data,10)                                       # using our correlation matrix visualization function\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good.  There is a mix of correlation magnitudes. Of course, correlation coeffficients are limited to degree of linear correlations.  For more complete information, let's look at the matrix scatter plot from the Pandas package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_plot.scatter_matrix(my_data, alpha = 0.1,                # pandas matrix scatter plot\n",
    "    figsize=(10, 10),color = 'black', hist_kwds={'color':['grey']})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Only Two Features\n",
    "\n",
    "Let's simplify the problem to 2 feature), Porosity and Brittleness to predict Production rate.  We will also reduce the number of wells from 1,000 to 500.  By working with only 2 features, it is very easy to visualize the segmentation of the feature space (it is only 2D and can be shown compleltely on a single plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_subset = my_data.iloc[:,[0,3,6]]                    # extract just por, brittle and prod with 300 training samples\n",
    "my_data_test_subset = my_data_test.iloc[:,[0,3,6]]          # extract just por, brittle and prod with 700 testing samples\n",
    "my_data_subset.describe().transpose()                       # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the univariate statistics of Porosity, Brittleness and Producton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True,figsize=(15,5)) # plot histograms\n",
    "ax1.hist(my_data_subset[\"Por\"], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "ax1.set_title('Porosity (%)')\n",
    "ax2.hist(my_data_subset[\"Brittle\"], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "ax2.set_title('Brittleness (%)')\n",
    "ax3.hist(my_data_subset[\"Production\"], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "ax3.set_title('Production (MCFPD)')\n",
    "prod_min = round(min(my_data_subset[\"Production\"]),0); prod_max = round(max(my_data_subset[\"Production\"]),0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions are well behaved, we cannot observe obvious gaps nor truncations.  Let's look at a scatter plot of Porosity vs. Brittleness with points colored by Production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))                                   # plot the training data\n",
    "im = plt.scatter(my_data_subset[\"Por\"],my_data_subset[\"Brittle\"],s=None, c=my_data_subset[\"Production\"], marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, verts=None, edgecolors=\"black\")\n",
    "plt.title('Production vs. Brittleness and Porosity'); plt.xlabel('Porosity (%)'); plt.ylabel('Brittleness (%)')\n",
    "cbar = plt.colorbar(im, orientation = 'vertical')\n",
    "cbar.set_label(\"Production\", rotation=270, labelpad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem looks complicated and could not be modeled with simple linear regression.  It appears there is a sweet spot for Brittleness and increasing Porosity is always beneficial for Production.\n",
    "\n",
    "#### Seperate Predictor and Response Feature DataFrames\n",
    "\n",
    "The input for the decision tree is 2 DataFrames, one with the predictors / features and the other with the response variable.  We will use the DataFrame member function copy() to copy the respective columns to 2 new DataFrames.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = my_data_subset[['Por','Brittle']].copy()       # make a new DataFrame with predictor features for training\n",
    "response = my_data_subset[['Production']].copy()            # make a new DataFrame with responses features for training\n",
    "predictors_test = my_data_test_subset[['Por','Brittle']].copy() # make a new DataFrame with predictor features for testing\n",
    "response_test = my_data_test_subset[['Production']].copy()  # make a new DataFrame with responses for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Tree Method - Tree Bagging Regression\n",
    "\n",
    "To perform tree bagging we will:\n",
    "\n",
    "1. set the hyperparameters for the individual trees\n",
    "\n",
    "```python\n",
    "seed = 73073\n",
    "max_depth = 100\n",
    "min_samples_leaf = 2  \n",
    "```\n",
    "\n",
    "2. instantiate an individual regression tree\n",
    "\n",
    "```python\n",
    "regressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf)\n",
    "```\n",
    "\n",
    "3. set the bagging hyperparameters\n",
    "\n",
    "```python\n",
    "num_trees = 100\n",
    "seed = 73073\n",
    "```\n",
    "\n",
    "4. instantiate the bagging regressor with the previously instantiated regression tree (wrapping the decision tree)\n",
    "\n",
    "```python\n",
    "bagging_model = BaggingRegressor(base_estimator=regressor, n_estimators=num_trees, random_state=seed)\n",
    "```\n",
    "\n",
    "5. train the bagging regression (wrapping the decision tree)\n",
    "\n",
    "\n",
    "```python\n",
    "bagging_model.fit(X = predictors, y = response)\n",
    "```\n",
    "\n",
    "6. visualize the model result over the feature space (easy to do as we have only 2 predictor features)\n",
    "\n",
    "#### Demonstration of Bagging by-Hand\n",
    "\n",
    "For demonstration let's set the number of trees to 1 and run 6 bagging regressors.  \n",
    "\n",
    "* the result for each is simply a complex decision tree\n",
    "\n",
    "* note, the random_state parameter is the random number seed for the bootstrap in the bagging method\n",
    "\n",
    "* the trees vary for each random number seed since the bootstrapped dataset will be different for each\n",
    "\n",
    "We will loop over the models and store each of them in an list of models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')                           # suppress warnings\n",
    "\n",
    "max_depth = 100; min_samples_leaf = 2                       # set for a complicated tree\n",
    "\n",
    "regressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) # instantiate a decision tree\n",
    "\n",
    "num_tree = 1                                                # use only a single tree for this demonstration\n",
    "seeds = [73073, 73074, 73075, 73076, 73077, 73078]\n",
    "bagging_models = []; oob_MSE = []; score = []; pred = []\n",
    "\n",
    "index = 1\n",
    "for seed in seeds:                                          # loop over random number seeds\n",
    "    bagging_models.append(BaggingRegressor(base_estimator=regressor, n_estimators=num_tree, random_state=seed, oob_score = True, n_jobs = 4))\n",
    "    bagging_models[index-1].fit(X = predictors, y = response)\n",
    "    oob_MSE.append(bagging_models[index-1].oob_score_)\n",
    "    score.append(bagging_models[index-1].score(X = predictors_test, y = response_test))\n",
    "    plt.subplot(2,3,index)\n",
    "    pred.append(visualize_model(bagging_models[index-1],my_data_subset[\"Por\"],pormin,pormax,my_data_subset[\"Brittle\"],brittlemin,brittlemax,my_data_subset[\"Production\"],prodmin,prodmax,'Training Data and Decision Tree Model #' + str(index) + ' '))\n",
    "    index = index + 1\n",
    "    \n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is significant misfit with the data \n",
    "\n",
    "* recall, in expectation, only 2/3 of the data are used for each tree\n",
    "\n",
    "Let's check the cross validation results with the withheld testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "for seed in seeds:                                          # loop over random number seeds\n",
    "    plt.subplot(2,3,index)\n",
    "    check_model(bagging_models[index-1],my_data_test_subset[\"Por\"],my_data_test_subset[\"Brittle\"],my_data_test_subset[\"Production\"],'Model Check Decision Tree #' + str(index))\n",
    "    index = index + 1\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's demonstrate the averaging of the predictions over the 6 decision trees.\n",
    "\n",
    "* we average the predicted response feature (production) over the discretized predictor feature space\n",
    "\n",
    "* we can take advantage of broadcast methods for operations on entire arrays\n",
    "\n",
    "* we will apply the same model check, but we will use a modified function to will read in the response feature 2D array, instead of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pred[0] \n",
    "index = 1\n",
    "for seed in seeds:                                          # loop over random number seeds\n",
    "    if index == 1:\n",
    "        Z = pred[index-1]\n",
    "    else:\n",
    "        Z = Z + pred[index-1]                               # calculate the average response over 3 trees\n",
    "    index = index + 1\n",
    "\n",
    "Z = Z / len(seeds)\n",
    "    \n",
    "plt.subplot(121)                                            # plot predictions over predictor feature space\n",
    "visualize_grid(Z,my_data_subset[\"Por\"],pormin,pormax,my_data_subset[\"Brittle\"],brittlemin,brittlemax,my_data_subset[\"Production\"],prodmin,prodmax,'Training Data - Average of 6 Bootstrapped Trees')\n",
    "\n",
    "plt.subplot(122)                                            # check model predictions vs. testing dataset\n",
    "check_grid(Z,pormin,pormax,brittlemin,brittlemax,my_data_test_subset[\"Por\"],my_data_test_subset[\"Brittle\"],my_data_test_subset[\"Production\"],'Model Check - Average of 6 Bootstrapped Trees')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.4, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made 6 complicated trees, each trained with bootstrap resamples of the original data and then averaged the predictions from each.\n",
    "\n",
    "* the result is more smooth - lower model variance \n",
    "\n",
    "* the result more closely matches the training data\n",
    "\n",
    "Note that the correlation and variance explained in cross validation is better than the average of the single tree models.\n",
    "\n",
    "* also notice that the cross plot is starting to concentrate over the 45 degree line\n",
    "\n",
    "* there are still some outliers, this would improved if we ran more trees, but this would be computationally inefficient by hand, let's use the built-in methods in scikit learn moving forward\n",
    "\n",
    "#### Demonstration of Bagging with Increasing Number of Trees\n",
    "\n",
    "For demonstration, let's build 6 bagging tree regression models with increasing number of overly complicated (and likely overfit) trees averaged.\n",
    "\n",
    "* with the bagging regressor from scikit learn this is automated with the 'num_tree' hyperparameter\n",
    "\n",
    "We will loop over the models and store each of them in an list of models again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Decision Tree Bagging Demonstration\n",
    "\n",
    "* select the inputs and observe the outputs of the decision trees and their model variance\n",
    "* interactive plot demonstration with ipywidgets, matplotlib packages\n",
    "\n",
    "#### Harry Khuc, Senior Undergraduate, The University of Texas at Austin\n",
    "\n",
    "##### [LinkedIn](https://www.linkedin.com/in/harry-khuc)\n",
    "\n",
    "#### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "\n",
    "##### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1) | [GeostatsPy](https://github.com/GeostatsGuy/GeostatsPy)\n",
    "\n",
    "### The Inputs\n",
    "\n",
    "Select the inputs for each function:\n",
    "\n",
    "* **Number of Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#WORKFLOW\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "max_depth = 3; min_samples_leaf = 5                         # set for a complicated tree\n",
    "\n",
    "regressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) # instantiate a decision tree\n",
    "\n",
    "seed = 73073;\n",
    "\n",
    "def update(a):\n",
    "    \n",
    "    num_trees = [*range(1, int(a)+1, 1)] # controlling number of trees\n",
    "    b = widgets.Tab(children = [out1, out2]) \n",
    "    \n",
    "    with out1:\n",
    "        #Original Code\n",
    "        bagging_models_ntrees = []\n",
    "        oob_MSE = []\n",
    "        score = []\n",
    "\n",
    "        index = 1\n",
    "        print('Complete of ' + str(len(num_trees)) + ': ', end =\" \")\n",
    "        if(len(num_trees) == 0):\n",
    "            print(\"No trees set yet\")   \n",
    "        else:\n",
    "            for num_tree in num_trees:                                  # loop over number of trees\n",
    "                bagging_models_ntrees.append(BaggingRegressor(base_estimator=regressor, n_estimators=num_tree, random_state=seed, oob_score = True, n_jobs = 4))\n",
    "                bagging_models_ntrees[index-1].fit(X = predictors, y = response)\n",
    "                oob_MSE.append(bagging_models_ntrees[index-1].oob_score_)\n",
    "                score.append(bagging_models_ntrees[index-1].score(X = predictors_test, y = response_test))\n",
    "                plt.subplot(3,3,index)\n",
    "                visualize_model(bagging_models_ntrees[index-1],my_data_subset[\"Por\"],pormin,pormax,my_data_subset[\"Brittle\"],brittlemin,brittlemax,my_data_subset[\"Production\"],prodmin,prodmax,'Training Data and Tree Model - ' + str(num_tree) + ' Tree(s)')\n",
    "                print(str(index)+ ', ', end =\" \")\n",
    "                index = index + 1\n",
    "        plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.2, hspace=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    with out2:        \n",
    "        index = 1\n",
    "        print('Complete of ' + str(len(num_trees)) + ': ', end =\" \")\n",
    "        if(len(num_trees) == 0):\n",
    "            print(\"No trees set yet\")\n",
    "        else:\n",
    "            for num_tree in num_trees:                                  # loop over number of trees\n",
    "                plt.subplot(3,3,index)\n",
    "                check_model(bagging_models_ntrees[index-1],my_data_test_subset[\"Por\"],my_data_test_subset[\"Brittle\"],my_data_test_subset[\"Production\"],'Model Check Bagging Tree, Number of Trees ' + str(num_trees[index-1]))\n",
    "                index = index + 1\n",
    "        plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTERACTION\n",
    "a = widgets.Dropdown(\n",
    "    options=['0','1', '2', '3','4','5','6','7','8','9'],\n",
    "    value='0',\n",
    "    description='# of Trees:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "out1 = widgets.Output()\n",
    "out2 = widgets.Output()\n",
    "\n",
    "b = widgets.Tab()\n",
    "b = widgets.Tab(children = [out1, out2])\n",
    "b.set_title(0, 'Tree Model')\n",
    "b.set_title(1, 'Cross Validation')\n",
    "\n",
    "interactive_plot = widgets.interactive_output(update, {'a': a})\n",
    "interactive_plot.clear_output(wait = True) \n",
    "%matplotlib inline\n",
    "display(a, b, interactive_plot)                            # display the interactive plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the impact of averaging an increasing number of trees. (Note: There will be a slight delay to generating the plots. You can check how much has progressed on the 'Tree Model' tab as the number of trees are completed.)\n",
    "\n",
    "* we transition from a discontinuous response prediction model to a continous model\n",
    "* you can see where the splits are made as there are more decision trees, and ultimately where a decision split has optimized the data\n",
    "\n",
    "In addition, you can check the modeling cross validation step with the withheld testing data on its tab. See the improvement with testing accuracy with increasing level of ensemble model averaging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive Model Variance vs. Ensemble Model Averaging\n",
    "\n",
    "Let's see the change in model variance through model averaging, we will compare multiple models with different numbers of trees averaged. This will be presented as 3 different seeds for 3 different number of trees. Select the number of trees for each row and then generate the plot by pressing the button.\n",
    "\n",
    "#### Harry Khuc, Senior Undergraduate, The University of Texas at Austin\n",
    "\n",
    "##### [LinkedIn](https://www.linkedin.com/in/harry-khuc)\n",
    "\n",
    "#### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "\n",
    "##### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1) | [GeostatsPy](https://github.com/GeostatsGuy/GeostatsPy)\n",
    "\n",
    "### The Inputs\n",
    "\n",
    "Select the inputs for each function:\n",
    "\n",
    "* **Number of Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "max_depth = 3; min_samples_leaf = 5                         # set for a complicated tree\n",
    "\n",
    "regressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) # instantiate a decision tree\n",
    "\n",
    "seeds = [73083, 73084, 73085]                               # number of random number seeds\n",
    "\n",
    "\n",
    "#INTERACTION\n",
    "a = widgets.Dropdown(                                       # number of trees averaged for each estimator row 1\n",
    "    options=['0','1', '2', '3','4','5','6','7','8','9'],\n",
    "    value='0',\n",
    "    description='# of Trees:',\n",
    "    disabled=False,\n",
    ")\n",
    "b = widgets.Dropdown(                                       # number of trees averaged for each estimator row 2\n",
    "    options=['0','1', '2', '3','4','5','6','7','8','9'],\n",
    "    value='0',\n",
    "    description='# of Trees:',\n",
    "    disabled=False,\n",
    ")\n",
    "c = widgets.Dropdown(                                       # number of trees averaged for each estimator row 3\n",
    "    options=['0','1', '2', '3','4','5','6','7','8','9'],\n",
    "    value='0',\n",
    "    description='# of Trees:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "out1 = widgets.Output()\n",
    "out2 = widgets.Output()\n",
    "\n",
    "d = widgets.Tab()                                           # tabs to show model variance and ensemble model\n",
    "d = widgets.Tab(children = [out1, out2])\n",
    "d.set_title(0, 'Tree Model')\n",
    "d.set_title(1, 'Cross Validation')\n",
    "\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Go',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(z):                                   #button to generate plots after selecting inputs\n",
    "    with output:\n",
    "        print(\"Generating Plots...\")\n",
    "        print('')\n",
    "        interactive_plot = widgets.interactive_output(update, {'a': a, 'b': b, 'c': c}) #plot auto-updates if inputs changed\n",
    "        interactive_plot.clear_output(wait = True)\n",
    "        display(interactive_plot)\n",
    "        \n",
    "%matplotlib inline\n",
    "display(a, b, c, button, output, d)                            # display the interactions\n",
    "button.on_click(on_button_clicked)                             # run the function upon pressing button\n",
    "\n",
    "\n",
    "#WORKFLOW\n",
    "def update(a, b, c):\n",
    "    \n",
    "    num_trees1 = int(a) # controlling number of trees\n",
    "    num_trees2 = int(b) # controlling number of trees\n",
    "    num_trees3 = int(c) # controlling number of trees\n",
    "    \n",
    "    total = 0                                                  # check how many trees will be generating\n",
    "    if num_trees1 > 0:\n",
    "        total = total + 1\n",
    "    if num_trees2 > 0:\n",
    "        total = total + 1\n",
    "    if num_trees3 > 0:\n",
    "        total = total + 1        \n",
    "    \n",
    "    b = widgets.Tab(children = [out1, out2]) \n",
    "    \n",
    "    bagging_models_ntrees_seeds = []\n",
    "    oob_MSE = []\n",
    "    score = []\n",
    "    \n",
    "    with out1:\n",
    "        print('Complete of ' + str(len(seeds)*total) + ': ', end =\" \") \n",
    "        \n",
    "        index = 1    \n",
    "        if(num_trees1 == 0):\n",
    "            print('')\n",
    "            print(\"No trees set yet\")\n",
    "        else:\n",
    "            for seed in seeds:                                      # loop over number of random number seeds\n",
    "                bagging_models_ntrees_seeds.append(BaggingRegressor(base_estimator=regressor, n_estimators=num_trees1, random_state=seed, oob_score = True, n_jobs = 4))\n",
    "                bagging_models_ntrees_seeds[index-1].fit(X = predictors, y = response)\n",
    "                oob_MSE.append(bagging_models_ntrees_seeds[index-1].oob_score_)\n",
    "                score.append(bagging_models_ntrees_seeds[index-1].score(X = predictors_test, y = response_test))\n",
    "                plt.subplot(3,3,index)\n",
    "                visualize_model(bagging_models_ntrees_seeds[index-1],my_data_subset[\"Por\"],pormin,pormax,my_data_subset[\"Brittle\"],brittlemin,brittlemax,my_data_subset[\"Production\"],prodmin,prodmax,'Training Data and Tree Model - ' + str(num_trees1) + ' Tree(s)')\n",
    "                print(str(index)+ ', ', end =\" \")\n",
    "                index = index + 1\n",
    "\n",
    "        index = 4\n",
    "        if(num_trees2 == 0):\n",
    "            print('')\n",
    "            print(\"No trees set yet\")\n",
    "        else:\n",
    "            for seed in seeds:                                      # loop over number of random number seeds\n",
    "                bagging_models_ntrees_seeds.append(BaggingRegressor(base_estimator=regressor, n_estimators=num_trees2, random_state=seed, oob_score = True, n_jobs = 4))\n",
    "                bagging_models_ntrees_seeds[index-1].fit(X = predictors, y = response)\n",
    "                oob_MSE.append(bagging_models_ntrees_seeds[index-1].oob_score_)\n",
    "                score.append(bagging_models_ntrees_seeds[index-1].score(X = predictors_test, y = response_test))\n",
    "                plt.subplot(3,3,index)\n",
    "                visualize_model(bagging_models_ntrees_seeds[index-1],my_data_subset[\"Por\"],pormin,pormax,my_data_subset[\"Brittle\"],brittlemin,brittlemax,my_data_subset[\"Production\"],prodmin,prodmax,'Training Data and Tree Model - ' + str(num_trees2) + ' Tree(s)')\n",
    "                print(str(index)+ ', ', end =\" \")\n",
    "                index = index + 1\n",
    "\n",
    "        index = 7                \n",
    "        if(num_trees3 == 0):\n",
    "            print('')\n",
    "            print(\"No trees set yet\")\n",
    "        else:\n",
    "            for seed in seeds:                                      # loop over number of random number seeds\n",
    "                bagging_models_ntrees_seeds.append(BaggingRegressor(base_estimator=regressor, n_estimators=num_trees3, random_state=seed, oob_score = True, n_jobs = 4))\n",
    "                bagging_models_ntrees_seeds[index-1].fit(X = predictors, y = response)\n",
    "                oob_MSE.append(bagging_models_ntrees_seeds[index-1].oob_score_)\n",
    "                score.append(bagging_models_ntrees_seeds[index-1].score(X = predictors_test, y = response_test))\n",
    "                plt.subplot(3,3,index)\n",
    "                visualize_model(bagging_models_ntrees_seeds[index-1],my_data_subset[\"Por\"],pormin,pormax,my_data_subset[\"Brittle\"],brittlemin,brittlemax,my_data_subset[\"Production\"],prodmin,prodmax,'Training Data and Tree Model - ' + str(num_trees3) + ' Tree(s)')\n",
    "                print(str(index)+ ', ', end =\" \")\n",
    "                index = index + 1\n",
    "                \n",
    "        plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=3.0, wspace=0.2, hspace=0.3) \n",
    "        plt.show()\n",
    "        \n",
    "    with out2:\n",
    "        print('Complete of ' + str(len(seeds)*total) + ': ', end =\" \") \n",
    "        print('')\n",
    "        \n",
    "        index = 1\n",
    "        if(num_trees1 == 0):\n",
    "            print('')\n",
    "            print(\"No trees set yet\")\n",
    "        else:\n",
    "            for seed in seeds:\n",
    "                plt.subplot(3,3,index)\n",
    "                check_model(bagging_models_ntrees_seeds[index-1],my_data_test_subset[\"Por\"],my_data_test_subset[\"Brittle\"],my_data_test_subset[\"Production\"],'Model Check Bagging Tree - ' + str(num_trees1) + ' Tree(s)')\n",
    "                index = index + 1\n",
    "                \n",
    "        index = 4\n",
    "        if(num_trees2 == 0):\n",
    "            print('')\n",
    "            print(\"No trees set yet\")\n",
    "        else:\n",
    "            for seed in seeds:\n",
    "                plt.subplot(3,3,index)\n",
    "                check_model(bagging_models_ntrees_seeds[index-1],my_data_test_subset[\"Por\"],my_data_test_subset[\"Brittle\"],my_data_test_subset[\"Production\"],'Model Check Bagging Tree - ' + str(num_trees2) + ' Tree(s)')\n",
    "                index = index + 1\n",
    "                \n",
    "        index = 7\n",
    "        if(num_trees3 == 0):\n",
    "            print('')\n",
    "            print(\"No trees set yet\")\n",
    "        else:\n",
    "            for seed in seeds:\n",
    "                plt.subplot(3,3,index)\n",
    "                check_model(bagging_models_ntrees_seeds[index-1],my_data_test_subset[\"Por\"],my_data_test_subset[\"Brittle\"],my_data_test_subset[\"Production\"],'Model Check Bagging Tree - ' + str(num_trees3) + ' Tree(s)')\n",
    "                index = index + 1\n",
    "                \n",
    "        plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=3.0, wspace=0.2, hspace=0.3) \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the number of decision trees averaged for the bagged tree regression models:\n",
    "\n",
    "* once again, the response predictions over the predictor feature space gets more smooth\n",
    "\n",
    "* the multiple realizations of the model start to converge, this is lower model variance\n",
    "\n",
    "Again, we have calculate cross validation variance explained for the above models:\n",
    "\n",
    "* We gain an improvement in cross validation with increasing number of trees\n",
    "\n",
    "* We will leave this with the above cross validation with withheld testing data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "With random forest for each split only $\\sqrt{m}$ predictor features are candidates. Note, in scikit learn the default is $\\frac{m}{3}$. Use this hyperparameter to set to square root of the number of predictor features.\n",
    "\n",
    "```python\n",
    "max_features = 'sqrt'\n",
    "```\n",
    "\n",
    "* this forces tree diversity / decorrelates the trees\n",
    "\n",
    "* recall the model variance reduced by averaging over multiple decision trees $Y = \\frac{1}{B} \\sum_{b=1}^{B} Y^b(X_1^b,...,X_m^b)$\n",
    "\n",
    "* recall from the [spatial bootstrap workflow](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Spatial_Bootstrap.ipynb) that correlation of samples being averaged attenuates the variance reduction\n",
    "\n",
    "Let's experiment with random forest to demonstrate this.  \n",
    "\n",
    "1. Set the hyperparameters.\n",
    "\n",
    "Even if I am just running one model, I set the random number seed to ensure I have a deterministic model, a model that can be rerun to get the same result everytime. If the random number seed is not set, then it is likely set based on the system time.\n",
    "\n",
    "```python\n",
    "seed = 73073\n",
    "```\n",
    "\n",
    "We will overtrain the trees, let them grow overly complicated. Once again, the ensemble approach will mitigate model variance and overfit.\n",
    "\n",
    "```python\n",
    "max_depth = 5\n",
    "```\n",
    "\n",
    "We will use a large number of trees to mitigate model variance and to benefit from random forest tree diversity.\n",
    "\n",
    "```python\n",
    "num_tree = 300\n",
    "```\n",
    "\n",
    "We are using a simple 2 predictor feature example for ease of visualization.  The default for scikit learn's random forest is to select $\\frac{m}{3}$  features at random for consideration for each split.  \n",
    "\n",
    "This doesn't make much sense when $m = 2$, as with our case, so we set the maximum number of features considered for each split to 1.  \n",
    "\n",
    "* We are forcing random selection of porosity or brittleness for consideration with each split, hierarchical binary segmentation.\n",
    "\n",
    "```python\n",
    "max_features = 1\n",
    "```\n",
    "2. Instantiate the random forest regressor with our hyperparameters  \n",
    "\n",
    "```python\n",
    "my_first_forest = RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features)\n",
    "```\n",
    "\n",
    "5. Train the random forest regression\n",
    "\n",
    "\n",
    "```python\n",
    "my_first_forest.fit(X = predictors, y = response)\n",
    "```\n",
    "\n",
    "6. Visualize the model result over the feature space (easy to do as we have only 2 predictor features)\n",
    "\n",
    "Let's build, visualize and cross validate our first random forest regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 73093                                                # set the random forest hyperparameters\n",
    "max_depth = 7\n",
    "num_tree = 300\n",
    "max_features = 1\n",
    "\n",
    "my_first_forest = RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features)\n",
    "\n",
    "my_first_forest.fit(X = predictors, y = response)           # train the model with training data \n",
    "\n",
    "plt.subplot(121)                                            # predict with the model over the predictor feature space and visualize\n",
    "visualize_model(my_first_forest,my_data_subset[\"Por\"],pormin,pormax,my_data_subset[\"Brittle\"],brittlemin,brittlemax,my_data_subset[\"Production\"],prodmin,prodmax,'Training Data and Random Forest Model')\n",
    "\n",
    "plt.subplot(122)                                            # perform cross validation with withheld testing data\n",
    "check_model(my_first_forest,my_data_test_subset[\"Por\"],my_data_test_subset[\"Brittle\"],my_data_test_subset[\"Production\"],'Model Check Random Forest Model')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.4, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of tree diversity!  We just built our best model so far.  \n",
    "\n",
    "* the conditional bias has decreased (our plot has a slope closer to 1:1)\n",
    "\n",
    "* we have the highest variance explained and linear correlation coefficient in testing with the withheld testing data\n",
    "\n",
    "Let's run some tests to make sure we understand random forest regression model.\n",
    "\n",
    "First let's confirm that only one feature (at random) is considered for each split\n",
    "\n",
    "* limit ourselves to maximum depth = 1, only one split\n",
    "\n",
    "* limit ourselves to a single tree in each forest!\n",
    "\n",
    "This way we can see the divesity in the first splits over multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')                           # ignore warnings\n",
    "                                  \n",
    "max_depth = 1                                               # set the random forest hyperparameters\n",
    "num_tree = 1\n",
    "max_features = 1\n",
    "simple_forest = []\n",
    "\n",
    "seeds = [73103,73104,73105,73106,73107,73108]               # set the random number seeds\n",
    "\n",
    "index = 1\n",
    "print('Complete of ' + str(len(seeds)) + ': ', end =\" \")\n",
    "for seed in seeds:                                          # loop over random number seeds\n",
    "    simple_forest.append(RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features))\n",
    "    simple_forest[index-1].fit(X = predictors, y = response)\n",
    "    plt.subplot(2,3,index)                               # predict with the model over the predictor feature space and visualize\n",
    "    visualize_model(simple_forest[index-1],my_data_subset[\"Por\"],pormin,pormax,my_data_subset[\"Brittle\"],brittlemin,brittlemax,my_data_subset[\"Production\"],prodmin,prodmax,'Training Data and Random Forest Model')\n",
    "    print(str(index)+ ', ', end =\" \")\n",
    "    index = index + 1\n",
    "    \n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.2, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first splits are 50/50 porosity and brittleness.  \n",
    "\n",
    "* aside, for all decision trees that I have fit to this dataset, porosity is always the feature selected for the first 2-3 levels of the tree.  \n",
    "\n",
    "* the random forest has resulted in model diversity by limiting the predictor features under consideration for the first split!\n",
    "\n",
    "Just incase you don't trust this, let's rerun the above code with both predictors allowed for all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "                                  \n",
    "max_depth = 1                                               # set the random forest hyperparameters\n",
    "num_tree = 1\n",
    "max_features = 2\n",
    "simple_forest = []\n",
    "\n",
    "seeds = [73103,73104,73105,73106,73107,73108]               # random number seeds \n",
    "\n",
    "index = 1\n",
    "print('Complete of ' + str(len(seeds)) + ': ', end =\" \")\n",
    "for seed in seeds:                                          # loop over random number seeds\n",
    "    simple_forest.append(RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features))\n",
    "    simple_forest[index-1].fit(X = predictors, y = response)\n",
    "    plt.subplot(2,3,index)                               # predict with the model over the predictor feature space and visualize\n",
    "    visualize_model(simple_forest[index-1],my_data_subset[\"Por\"],pormin,pormax,my_data_subset[\"Brittle\"],brittlemin,brittlemax,my_data_subset[\"Production\"],prodmin,prodmax,'Training Data and Random Forest Model')\n",
    "    print(str(index)+ ', ', end =\" \")\n",
    "    index = index + 1\n",
    "    \n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a set of first splits that vary (due to the bootstrap of the training data), but are all over porosity.\n",
    "\n",
    "#### Model Performance by Out-of-Bag and Feature Importance\n",
    "\n",
    "Since we are now building a more robust model with a large ensemble of trees, let's get more serious about model checking.\n",
    "\n",
    "* we will look at out-of-bag mean square error\n",
    "\n",
    "* we will look at feature importance\n",
    "\n",
    "Let's start with a pretty big forest, this may take a while to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 73093                                                # set the random forest hyperparameters\n",
    "max_depth = 7\n",
    "num_tree = 500\n",
    "max_features = 1\n",
    "\n",
    "big_forest = RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features, oob_score = True, n_jobs = 4)\n",
    "\n",
    "big_forest.fit(X = predictors, y = response)\n",
    "\n",
    "plt.subplot(121)                                            # predict with the model over the predictor feature space and visualize\n",
    "visualize_model(big_forest,my_data_subset[\"Por\"],pormin,pormax,my_data_subset[\"Brittle\"],brittlemin,brittlemax,my_data_subset[\"Production\"],prodmin,prodmax,'Training Data and Random Forest Model')\n",
    "\n",
    "plt.subplot(122)                                            # perform cross validation with withheld testing data\n",
    "check_model(big_forest,my_data_test_subset[\"Por\"],my_data_test_subset[\"Brittle\"],my_data_test_subset[\"Production\"],'Model Check Random Forest Model')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.4, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the feature importance we just have to access the model member 'feature_importance_'.\n",
    "\n",
    "* we had to set feature_importance to true in the model instantiation for this to be avaible\n",
    "\n",
    "* this measure is standardized to sum to 1.0\n",
    "\n",
    "* same order as the predictor features in the 2D array, porosity and then brittleness\n",
    "\n",
    "* feature importance is the proportion of total MSE reduction through splits for each feature\n",
    "\n",
    "* we can access the importance for each feature for each tree in the forest or the global average for each over the entire forest\n",
    "\n",
    "We get the global average of feature importance with this member of the random forest regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_forest.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot this with a bar chart and include the error bar using the feature importance from every tree in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = big_forest.feature_importances_               # expected (global) importance over the forest fore each predictor feature\n",
    "std = np.std([tree.feature_importances_ for tree in big_forest.estimators_],axis=0) # retrieve importance by tree\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "features = ['Porosity','Brittleness']                       # names or predictor features\n",
    "\n",
    "for f in range(len(features)):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "plt.subplot(111)\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(features, importances[indices],color=\"red\", alpha = 0.2, edgecolor = 'black', yerr=std[indices], align=\"center\")\n",
    "#plt.xticks(range(X.shape[1]), indices)\n",
    "plt.ylim(0,1), plt.xlabel('Predictor Features'); plt.ylabel('Feature Importance')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some hyperparameter training with the out-of-bag mean square error measure from our forest.\n",
    "\n",
    "Let's start with the number of trees in our forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "                                  \n",
    "max_depth = 5                                              # set the random forest hyperparameters\n",
    "num_trees = np.linspace(1,100,100)\n",
    "max_features = 1\n",
    "fit_forests = []\n",
    "oob_MSE = []\n",
    "\n",
    "index = 1\n",
    "print('Complete of ' + str(len(num_trees)) + ': ', end =\" \")\n",
    "for num_tree in num_trees:                                 # loop over number of trees in our random forest\n",
    "    fit_forests.append(RandomForestRegressor(oob_score = True,max_depth=max_depth, random_state=seed,n_estimators=int(num_tree), max_features=max_features))\n",
    "    fit_forests[index-1].fit(X = predictors, y = response)\n",
    "    oob_MSE.append(fit_forests[index-1].oob_score_)\n",
    "    print(str(index)+ ', ', end =\" \")\n",
    "    index = index + 1\n",
    "    \n",
    "plt.subplot(121)\n",
    "plt.scatter(num_trees,oob_MSE,s=None, c='red', marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=0.3, verts=None, edgecolors=\"black\")\n",
    "plt.title('Out of Bag Score vs. Number of Trees'); plt.xlabel('Number of Trees Averaged'); plt.ylabel('Out-of-Bag Variance Explained')\n",
    "plt.xlim(0,100); plt.ylim(0,1.0)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.2, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the depth of the trees, given enough trees (we'll use 60 trees) as determined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "                                  \n",
    "max_depths = np.linspace(1,40,40)                          # set the tree maximum tree depths to consider\n",
    "\n",
    "num_tree = 60                                              # set the random forest hyperparameters\n",
    "max_features = 1\n",
    "fit_forests = []\n",
    "oob_MSE = []\n",
    "\n",
    "index = 1\n",
    "print('Complete of ' + str(len(max_depths)) + ': ', end =\" \")\n",
    "for max_depth in max_depths:                               # loop over tree depths\n",
    "    fit_forests.append(RandomForestRegressor(oob_score = True,max_depth=int(max_depth), random_state=seed,n_estimators=num_tree, max_features=max_features))\n",
    "    fit_forests[index-1].fit(X = predictors, y = response)\n",
    "    oob_MSE.append(fit_forests[index-1].oob_score_)\n",
    "    print(str(index)+ ', ', end =\" \")\n",
    "    index = index + 1\n",
    "    \n",
    "plt.subplot(121)                                           # plot the variance explained vs. tree depth\n",
    "plt.scatter(max_depths,oob_MSE,s=None, c='red', marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=0.3, verts=None, edgecolors=\"black\")\n",
    "plt.title('Out of Bag Score vs. Tree Maximum Depth'); plt.xlabel('Maximum Tree Depth'); plt.ylabel('Out-of-Bag Variance Explained')\n",
    "plt.xlim(0,40); plt.ylim(0,1.0)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.2, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we need a maximum tree depth of atleast 10 splits for best performance of our model with respect to out-of-bag samples variance explained.\n",
    "\n",
    "* note that our model is robust and resistant to overfit, the out-of-bag performance evaluation is close to monotonically increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "This was an interactive workflow covering the basics of tree bagging and ensemble tree averaging. \n",
    "\n",
    "The Texas Center for Geostatistics has many other demonstrations on the basics of working with DataFrames, ndarrays, univariate statistics, plotting data, declustering, data transformations, trend modeling and many other workflows available [here](https://github.com/GeostatsGuy/PythonNumericalDemos), along with a package for geostatistics in Python called [GeostatsPy](https://github.com/GeostatsGuy/GeostatsPy). \n",
    "  \n",
    "We hope this was helpful,\n",
    "\n",
    "*Harry Khuc* and *Michael*\n",
    "\n",
    "***\n",
    "\n",
    "#### More on Michael Pyrcz and the Texas Center for Data Analytics and Geostatistics:\n",
    "\n",
    "### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "*Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions*\n",
    "\n",
    "With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers' and geoscientists' impact in subsurface resource development. \n",
    "\n",
    "For more about Michael check out these links:\n",
    "\n",
    "#### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
    "\n",
    "#### Want to Work Together?\n",
    "\n",
    "I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.\n",
    "\n",
    "* Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I'd be happy to drop by and work with you! \n",
    "\n",
    "* Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!\n",
    "\n",
    "* I can be reached at mpyrcz@austin.utexas.edu.\n",
    "\n",
    "I'm always happy to discuss,\n",
    "\n",
    "*Michael*\n",
    "\n",
    "Michael Pyrcz, Ph.D., P.Eng. Associate Professor The Hildebrand Department of Petroleum and Geosystems Engineering, Bureau of Economic Geology, The Jackson School of Geosciences, The University of Texas at Austin\n",
    "\n",
    "#### More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
